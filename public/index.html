<!DOCTYPE html>
<html lang="en-us">

<head>
  

  
  


  <meta charset="utf-8">



<meta name="viewport" content="width=device-width, initial-scale=1.0">


  
    
  


<meta name="color-scheme" content="light dark">







<meta name="generator" content="Hugo 0.121.2">
  <title>AI Dungeons - Everything I learned about AI</title>
  <link rel="canonical" href="https://ai.dungeons.ca/">


  <link rel="icon" href="/favicon.ico" type="image/x-icon">



  <link rel="alternate" href="/index.xml" type="application/rss+xml">

  








  

  
    
    
    
    
    
  
    
    
    
    
    
  
    
    
    
    
    
  
    
    
    
      
      
    
    
    
  
    
    
    
      
      
    
    
    
  
    
    
    
      
      
    
    
    
  
    
    
    
      
      
    
    
    
  
    
    
    
      
      
    
    
    
  
    
    
    
      
      
    
    
    
  
    
    
    
      
      
    
    
    
  


  
  <link rel="stylesheet" href="/css/base.min.ee1d0b98bb68d9e71b2feee16bec52548b2bc0c3f58301d404729345cf0788e3.css" integrity="sha256-7h0LmLto2ecbL&#43;7ha&#43;xSVIsrwMP1gwHUBHKTRc8HiOM=" crossorigin="anonymous">



</head>

<body>
  <nav class="u-background">
  <div class="u-wrapper">
    <ul class="Banner">
      <li class="Banner-item Banner-item--title">
        <h1 class="Banner-heading">
          <a class="Banner-link u-clickable" href="/">AI Dungeons - Everything I learned about AI</a>
        </h1>
      </li>
      
        
        
        <li class="Banner-item">
          <a class="Banner-link u-clickable" href="/about/">About</a>
        </li>
      
        
        
        <li class="Banner-item">
          <a class="Banner-link u-clickable" href="/posts/">Posts</a>
        </li>
      
        
        
        <li class="Banner-item">
          <a class="Banner-link u-clickable" href="/tags/">Tags</a>
        </li>
      
    </ul>
  </div>
</nav>

  <main>
    <div class="u-wrapper">
      <div class="u-padding">
        

  
  
    <article>
      <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="/posts/tool-series-extbrain/" rel="bookmark">Tool Series - extBrain</a>
  </h2>
  
    <time datetime="2024-02-12T00:00:00Z">12 February, 2024</time>
  
</header>
      
        <h1 id="tool-series---extbrain-your-external-brain-for-building-another-you">
  <a class="Heading-link u-clickable" href="/posts/tool-series-extbrain/#tool-series---extbrain-your-external-brain-for-building-another-you">Tool Series - extBrain: Your External Brain for Building Another You</a>
</h1>
<p><a href="https://github.com/patw/ExternalBrain">https://github.com/patw/ExternalBrain</a></p>
<p>In our ongoing series exploring various tools to build Generative AI applications, we present the eighth tool in the lineup: <em>extBrain</em>. This innovative platform allows you to tap into a powerful knowledge management system designed specifically for question answering. By leveraging advanced techniques such as Fact Synthesis and Retrieval Augmented Generation (RAG), extBrain enables efficient storage, vector storage, and precise semantic search capabilities. In this blog post, we&rsquo;ll delve into the technical aspects of <em>extBrain</em> and how it can transform your AI application development process.</p>
<h2 id="a-brief-overview-of-fact-synthesis">
  <a class="Heading-link u-clickable" href="/posts/tool-series-extbrain/#a-brief-overview-of-fact-synthesis">A Brief Overview of Fact Synthesis</a>
</h2>
<p>At the core of extBrain lies its ability to break down large text articles into individual facts using Fact Synthesis. This process involves reducing textual data into a structured format (facts), making it easier for machines to understand and process information. By storing these facts in Mongo collections alongside metadata such as the source, context, and timestamp, extBrain can provide authoritative answers based on up-to-date knowledge sources.</p>
<h2 id="grouping-facts-into-semantically-relevant-chunks">
  <a class="Heading-link u-clickable" href="/posts/tool-series-extbrain/#grouping-facts-into-semantically-relevant-chunks">Grouping Facts into Semantically Relevant Chunks</a>
</h2>
<p>One of the key advantages of using extBrain is its ability to group facts into chunks of text. This can be achieved through various methods, including fixed numbers of grouped facts or more advanced techniques like context-based grouping and semantic similarity matching. By organizing facts into relevant groups, extBrain ensures that users receive accurate responses tailored to their specific inquiries.</p>
<h2 id="leveraging-rag-for-enhanced-question-answering">
  <a class="Heading-link u-clickable" href="/posts/tool-series-extbrain/#leveraging-rag-for-enhanced-question-answering">Leveraging RAG for Enhanced Question Answering</a>
</h2>
<p>RAG (Retrieval Augmented Generation) systems  are common in the AI industry. These systems break down source documents such as PDF, Word, or HTML files into smaller text blobs and perform text embedding to generate dense vectors. ExtBrain takes this concept one step further by focusing exclusively on grouping facts together, resulting in a more efficient use of resources and improved vector search accuracy.</p>
<h2 id="using-semantically-similar-facts-for-larger-text-chunks">
  <a class="Heading-link u-clickable" href="/posts/tool-series-extbrain/#using-semantically-similar-facts-for-larger-text-chunks">Using Semantically Similar Facts for Larger Text Chunks</a>
</h2>
<p>By grouping semantically similar facts into larger text chunks, extBrain enables users to ask broader questions while still maintaining high recall rates. This approach ensures that relevant information is readily available without compromising the overall efficiency of the system.</p>
<h2 id="a-second-you-the-power-of-your-external-brain">
  <a class="Heading-link u-clickable" href="/posts/tool-series-extbrain/#a-second-you-the-power-of-your-external-brain">A Second You: The Power of Your External Brain</a>
</h2>
<p>ExtBrain&rsquo;s primary goal is to act as an extension of your own knowledge and cognitive abilities. By ingesting all your relevant data, extBrain can provide authoritative answers on demand, allowing you to work smarter instead of harder. This innovative approach to AI application development has the potential to revolutionize how we approach problem-solving and information retrieval.</p>
<h2 id="multiple-front-end-options-for-easy-access">
  <a class="Heading-link u-clickable" href="/posts/tool-series-extbrain/#multiple-front-end-options-for-easy-access">Multiple Front End Options for Easy Access</a>
</h2>
<p>To make it even more convenient for users, extBrain offers multiple front end options for asking questions, including a website, a Discord bot, and a Slack bot. These intuitive interfaces allow you to access your external brain from virtually anywhere, making it simple to retrieve accurate information whenever you need it.</p>
<h2 id="managing-facts-and-summarizing-text-with-the-extbrain-back-end">
  <a class="Heading-link u-clickable" href="/posts/tool-series-extbrain/#managing-facts-and-summarizing-text-with-the-extbrain-back-end">Managing Facts and Summarizing Text with the extBrain Back End</a>
</h2>
<p>The extBrain back end serves as an administrative interface for managing facts and summarizing large chunks of text into digestible pieces of information. This powerful tool enables users to input, organize, and refine their knowledge base, ensuring that they always have access to up-to-date, accurate data when needed.</p>
<h2 id="conclusion-why-everyone-should-have-an-external-brain">
  <a class="Heading-link u-clickable" href="/posts/tool-series-extbrain/#conclusion-why-everyone-should-have-an-external-brain">Conclusion: Why Everyone Should Have an External Brain</a>
</h2>
<p>In conclusion, extBrain offers a comprehensive solution for building scalable AI applications focused on question answering. With its innovative approach to Fact Synthesis and RAG systems, combined with the ability to group semantically similar facts into larger text chunks, extBrain delivers efficient storage solutions and accurate vector search capabilities. By leveraging these features, you can work smarter, not harder, and revolutionize your AI application development process. So why wait? It&rsquo;s time to unlock the power of your external brain!</p>
<p><em>Note: The views expressed in this blog post are based on publicly available information and represent our understanding of extBrain as a tool for building Generative AI applications.</em></p>
<ul>
<li>Human Intervention: None</li>
</ul>
<h3 id="facts-used">
  <a class="Heading-link u-clickable" href="/posts/tool-series-extbrain/#facts-used">Facts Used:</a>
</h3>
<ul>
<li>This series covers different tools used for building Generative AI (genai) applications</li>
<li>Eighth Tool in the series is External Brain (<a href="https://github.com/patw/ExternalBrain">https://github.com/patw/ExternalBrain</a>)</li>
<li>External Brain uses Fact Synthesis (reducing text into facts - see previous posts about the subject) which allows the user to paste in large text articles and reduce them down to individual facts.</li>
<li>Facts are stored in Mongo collections along with some metadata about who stated the facts, what was the context and when it was stated.</li>
<li>The facts can then be grouped together into chunks of text, by either a fixed number of grouped facts or with more clever methods like only grouping together facts from the same context or even facts based on their semantic similarity.</li>
<li>Users can then ask questions to the LLM (large language model) and we will perform semantic search to find the fact chunks that are most relevant to the question, which will be injected into the prompt to the LLM.  This is an example of RAG (retrieval augmented generation).</li>
<li>Most RAG systems will chunk up source documents like PDF, Word, HTML files into blobs of text, and run text embedding models against that to produce dense vectors.</li>
<li>extBrain only chunks on groups of facts, which reduces the overall size of the data set and produces more accurate results in vector search and stronger results from the LLM.</li>
<li>Semantically similar facts can produce larger text chunks without causing recall issues later on.</li>
<li>External Brain can act as a second you.  It’s designed to ingest all your knowledge and be able to answer authoritatively on it.  This technique could allow knowledge workers to scale.</li>
<li>extBrain has multiple front ends for asking questions:  A website, a Discord bot and a Slack bot.</li>
<li>The extBrain back end, or admin UI allows you to enter and manage facts and summarize large chunks of text into facts.</li>
<li>This is a pretty ideal system for question answering.  It’s efficient on storage and vectors (which are expensive to store).</li>
<li>Call to action:  Everyone should have an external brain!  Work smarter not harder.</li>
</ul>

      
      


  

  





  <footer>
    
      
        <ul class="Tags">
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/categories/ai/" rel="tag">AI</a>
            </li>
          
        </ul>
      
    
      
        <ul class="Tags">
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/ai-tools/" rel="tag">AI Tools</a>
            </li>
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/vector-search/" rel="tag"> Vector Search</a>
            </li>
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/python/" rel="tag"> Python</a>
            </li>
          
        </ul>
      
    
  </footer>

    </article>
    
      <div class="Divider"></div>
    
  
    <article>
      <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="/posts/tool-series-factweave/" rel="bookmark">Tool Series - FactWeave</a>
  </h2>
  
    <time datetime="2024-02-09T00:00:00Z">9 February, 2024</time>
  
</header>
      
        <h1 id="tool-series---factweave-writing-more-by-writing-less">
  <a class="Heading-link u-clickable" href="/posts/tool-series-factweave/#tool-series---factweave-writing-more-by-writing-less">Tool Series - FactWeave: Writing More by Writing Less</a>
</h1>
<p>In the ongoing series of covering different tools for building Generative AI applications, we introduce FactWeave (<a href="https://github.com/patw/FactWeave)">https://github.com/patw/FactWeave)</a>, a unique tool that can generate blog posts with minimal input. Designed with Fact Expansion in mind, it&rsquo;s the perfect solution if you want to share your thoughts and ideas without spending hours crafting each post.</p>
<h2 id="how-factweave-works">
  <a class="Heading-link u-clickable" href="/posts/tool-series-factweave/#how-factweave-works">How FactWeave Works</a>
</h2>
<p>FactWeave is an incredible example of Fact Expansion. This tool allows you to input individual facts, which are then fed into a Large Language Model (LLM) along with some prompt engineering to generate specific types of blog posts. You can choose from technical, personal, or humor-based content, and the output will be Markdown files ready for consumption by static site generators like HUGO.</p>
<h2 id="a-website-built-with-factweave">
  <a class="Heading-link u-clickable" href="/posts/tool-series-factweave/#a-website-built-with-factweave">A Website Built with FactWeave</a>
</h2>
<p>The website you&rsquo;re currently reading is a testament to the power of FactWeave. This tool has helped me share my ideas in a well-formatted and professional manner, which has been incredibly valuable for my customers at Mongodb who are interested in building Retrieval Augmented Generation (RAG) use cases.</p>
<h2 id="managing-your-blog-posts-with-factweave">
  <a class="Heading-link u-clickable" href="/posts/tool-series-factweave/#managing-your-blog-posts-with-factweave">Managing Your Blog Posts with FactWeave</a>
</h2>
<p>FactWeave also acts as a Content Management System (CMS). It enables you to manage your blog posts, change their tags, titles, or content. If the &ldquo;post&rdquo; field is blank, FactWeave will generate the blog post using the LLM. Afterward, you can edit the post as needed.</p>
<h2 id="tailoring-the-content">
  <a class="Heading-link u-clickable" href="/posts/tool-series-factweave/#tailoring-the-content">Tailoring the Content</a>
</h2>
<p>The default tone for FactWeave&rsquo;s generated posts is technical, detailed, and professional. However, you have the option to change this tone to personal if you prefer a more casual writing style. The term &ldquo;detailed&rdquo; might sometimes produce overly wordy content, so you can also switch it to &ldquo;succinct&rdquo; for shorter blog posts.</p>
<h2 id="automating-tags">
  <a class="Heading-link u-clickable" href="/posts/tool-series-factweave/#automating-tags">Automating Tags</a>
</h2>
<p>Currently, FactWeave uses default tags such as &ldquo;RAG&rdquo;, &ldquo;Grounding&rdquo;, and &ldquo;LLM&rdquo;. However, I plan on updating the system to generate these tags automatically based on the content of each post. This way, the AI can help you categorize your blog posts more effectively!</p>
<h2 id="factweave-built-with-python">
  <a class="Heading-link u-clickable" href="/posts/tool-series-factweave/#factweave-built-with-python">FactWeave Built With Python</a>
</h2>
<p>FactWeave is built using Flask, FlaskBootstrap, and FlaskForms, which are popular tools in this series. The tool also incorporates vector search functionality to help you find relevant articles for editing later.</p>
<h2 id="conclusion">
  <a class="Heading-link u-clickable" href="/posts/tool-series-factweave/#conclusion">Conclusion</a>
</h2>
<p>In conclusion, FactWeave is a powerful tool that can help you create engaging blog posts with minimal effort. By inputting individual facts, the AI generates well-formatted, personalized content that can be edited and managed as needed. With its versatility and ease of use, FactWeave is an excellent choice for anyone looking to streamline their blogging process.</p>
<p>So why not give it a try? Write more by writing less with FactWeave!</p>
<ul>
<li>Human Intervention: Minor.  It called Fact Expansion &ldquo;Facet Expansion&rdquo;.  Also renamed one of the headings.</li>
</ul>
<h3 id="facts-used">
  <a class="Heading-link u-clickable" href="/posts/tool-series-factweave/#facts-used">Facts Used:</a>
</h3>
<ul>
<li>This series covers different tools used for building Generative AI (genai) applications</li>
<li>Seventh Tool in the series is FactWeave (<a href="https://www.github.com/patw/FactWeave">https://www.github.com/patw/FactWeave</a>)</li>
<li>FactWeave is a tool for generating blog posts by just providing individual facts.</li>
<li>This tool is an example of Fact Expansion (see previous blog post about this subject), it&rsquo;s the opposite of Fact Synthesis</li>
<li>You can provide an arbitrary number of facts, which get fed to the LLM (large language model), with some prompt engineering to produce a specific type of blog post (technical, personal, humor) and output Markdown files.</li>
<li>The Markdown files can be consumed by static site generators like HUGO, to deploy a complete website with minimal inputs.</li>
<li>The website you&rsquo;re reading right now is produced by FactWeave</li>
<li>FactWeave helped me get my ideas into a nicely formatted blog site which has been valuable for my customers at MongoDB who are interested in building RAG (retrieval augmented generation) use cases</li>
<li>FactWeave itself is a CMS.  It provides the ability to manage the blog posts, change their tags, title or content.  It only generates the blog post using the LLM if the “post” field is blank.  After the post is generated it can be edited.</li>
<li>Sometimes I do need to edit the blog posts and I indicate that with the “Human Intervention” part at the bottom of the post.  If it says “None”, it means I haven’t edited the post.  If I do have to edit I explain what and why.  Usually it’s due to the LLM hallucinating URLs or referencing open source projects with the wrong name.</li>
<li>The default tone for posts is “technical, detailed and professional”.  This directs the LLM to produce technical sounding blogs.  I sometimes change &ldquo;professional to “personal” when I want that tone instead.  The term “detailed” can also be a problem, sometimes.  It’ll get very wordy, so I’ll change it to “succinct” instead.</li>
<li>The system also has default tags “RAG, Grounding, LLM” but I’ll modify the system later to have to produce tags automatically from the outputted content.  When you have an AI problem, more AI fixes it!</li>
<li>The tool is built using Flask, FlaskBootstrap and FlaskForms as are many of the tools in this series.</li>
<li>It also incorporates vector search, to find relevant articles.  This is so I can edit them later.</li>
<li>Call to action:  This same technique could be used for building technical documentation, or even your own blogging solution.  Clever tagline: Write more by writing less!</li>
</ul>

      
      


  

  





  <footer>
    
      
        <ul class="Tags">
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/categories/ai/" rel="tag">AI</a>
            </li>
          
        </ul>
      
    
      
        <ul class="Tags">
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/ai-tools/" rel="tag">AI Tools</a>
            </li>
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/vector-search/" rel="tag"> Vector Search</a>
            </li>
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/python/" rel="tag"> Python</a>
            </li>
          
        </ul>
      
    
  </footer>

    </article>
    
      <div class="Divider"></div>
    
  
    <article>
      <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="/posts/-tool-series-discord_llama/" rel="bookmark"> Tool Series - discord_llama</a>
  </h2>
  
    <time datetime="2024-02-08T00:00:00Z">8 February, 2024</time>
  
</header>
      
        <h1 id="tool-series-discord_llama---the-ultimate-ai-companion-for-your-server">
  <a class="Heading-link u-clickable" href="/posts/-tool-series-discord_llama/#tool-series-discord_llama---the-ultimate-ai-companion-for-your-server">Tool Series: Discord_Llama - The Ultimate AI Companion for Your Server</a>
</h1>
<p>Discord has become an integral part of our online social lives, allowing friends and communities to connect through chat rooms, voice channels, and even play games together. With the ever-growing demand for unique experiences in these servers, it&rsquo;s no wonder that developers are continuously creating innovative tools to enhance interactions between users. Today, we&rsquo;re taking a deep dive into <code>discord_llama</code>, a fantastic tool designed to bring large language models (LLMs) to life within your Discord server.</p>
<p><code>Discord_Llama</code> is an open-source project by <a href="https://github.com/patw">Pat W.</a> that allows you to create LLM-driven chatbots tailored to your server&rsquo;s needs. This versatile tool can introduce personality, humor, and even specific ideologies into your bot, making for a more engaging and entertaining user experience.</p>
<h2 id="how-does-it-work">
  <a class="Heading-link u-clickable" href="/posts/-tool-series-discord_llama/#how-does-it-work">How Does it Work?</a>
</h2>
<p><code>Discord_Llama</code> leverages the same <code>llama.cpp</code> running in server mode as its LLM backend, sharing this powerful technology with other tools such as BottyBot, SumBot, RAGTAG, and ExtBrain. This backend is GPU-accelerated, ensuring lightning-fast responses to user queries on your Discord server.</p>
<p>Currently, <code>discord_llama</code> supports up to 7 different personality-based bots, ranging from the more conventional WizaardBot for answering questions to more unique concepts like ideology-focused bots or even a bot that clones a friend and their interests (HermanBot). The tool offers a wide variety of options to cater to your server&rsquo;s preferences.</p>
<h2 id="chatting-with-bots">
  <a class="Heading-link u-clickable" href="/posts/-tool-series-discord_llama/#chatting-with-bots">Chatting with Bots</a>
</h2>
<p>One standout feature of <code>discord_llama</code> is its access to Discord channel history, allowing bots to engage in back-and-forth conversations with users for up to five lines by default. This immersive interaction greatly enhances the user experience and fosters a more natural conversation flow within your server.</p>
<h2 id="the-future-of-discord_llama">
  <a class="Heading-link u-clickable" href="/posts/-tool-series-discord_llama/#the-future-of-discord_llama">The Future of <code>discord_llama</code></a>
</h2>
<p>Although development on <code>discord_llaama</code> has slowed in recent months, it continues to provide valuable chatbot services to four different Discord servers. A potential enhancement for this tool could involve further augmentation, such as web searches for up-to-date information or more advanced interaction capabilities. Regardless of its future developments, <code>discord_llaama</code> remains an invaluable asset for server owners and users alike.</p>
<h2 id="conclusion">
  <a class="Heading-link u-clickable" href="/posts/-tool-series-discord_llama/#conclusion">Conclusion</a>
</h2>
<p>In a world where Discord has become the go-to platform for connecting with friends and communities, having engaging chatbots is more important than ever. With <code>Discord_Llama</code>, you can now add personality and unique experiences to your servers, enriching conversations and entertaining users in ways previously unimaginable. If you haven&rsquo;t already given this tool a try, I highly recommend checking out the GitHub repository <a href="https://www.github.com/patw/discord_llama">here</a> and exploring the potential of LLMs in your Discord community. Who knows? You might just discover your server&rsquo;s new best friend!</p>
<ul>
<li>Human Intervention: None</li>
</ul>
<h3 id="facts-used">
  <a class="Heading-link u-clickable" href="/posts/-tool-series-discord_llama/#facts-used">Facts Used:</a>
</h3>
<ul>
<li>This series covers different tools used for building Generative AI (genai) applications</li>
<li>
<ul>
<li>Sixth Tool in the series is discord_llama (<a href="https://www.github.com/patw/discord_llama">https://www.github.com/patw/discord_llama</a>)</li>
</ul>
</li>
<li>
<ul>
<li>discord_llama is a tool for building LLM (large language model) driven chatbots for Discord servers</li>
</ul>
</li>
<li>
<ul>
<li>The tool allows you to build bots with personality and add them to an existing discord server.  They can respond to questions, and even react to specific keywords, at random, to act like regular users in the discord server.</li>
</ul>
</li>
<li>
<ul>
<li>discord_llama uses the same llama.cpp running in server mode as the LLM back end which is shared in my homelab with tools like BottyBot, SumBot, RAGTAG and ExtBrain.  This llama.cpp instance is GPU accelerated allowing very fast responses to questions from users in Discord.</li>
</ul>
</li>
<li>
<ul>
<li>I currently run about 7 different personality based bots that range from the very boring WizardBot, which is a typical chatbot for answering questions to more extreme personality based bots like ideological focused bots and even a bot to clone a friend and his interests (HermanBot)!</li>
</ul>
</li>
<li>
<ul>
<li>The bots have access to the discord channel history (up to 5 lines by default) which allows them to have back and forth exchanges with Discord users, which provides a great experience for the users.</li>
</ul>
</li>
<li>
<ul>
<li>This project hasn&rsquo;t seen much work in the last few months, but continues to provide useful chatbot services to 4 different Discord servers.</li>
</ul>
</li>
<li>
<ul>
<li>A future enhancement to this tool could be further augmentation like web searches, for more up to date information.</li>
</ul>
</li>
<li>
<ul>
<li>If you run a discord server, or particiopate in one, this tool can add a ton of value to conversations, or just troll the users <!-- raw HTML omitted --></li>
</ul>
</li>
</ul>

      
      


  

  





  <footer>
    
      
        <ul class="Tags">
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/categories/ai/" rel="tag">AI</a>
            </li>
          
        </ul>
      
    
      
        <ul class="Tags">
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/discord/" rel="tag">Discord</a>
            </li>
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/llm/" rel="tag"> LLM</a>
            </li>
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/chatbots/" rel="tag"> Chatbots</a>
            </li>
          
        </ul>
      
    
  </footer>

    </article>
    
      <div class="Divider"></div>
    
  
    <article>
      <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="/posts/tool-series-bottybot/" rel="bookmark">Tool Series - BottyBot</a>
  </h2>
  
    <time datetime="2024-02-08T00:00:00Z">8 February, 2024</time>
  
</header>
      
        <h2 id="tool-series---bottybot-a-frontend-chat-ui-for-local-llm-models">
  <a class="Heading-link u-clickable" href="/posts/tool-series-bottybot/#tool-series---bottybot-a-frontend-chat-ui-for-local-llm-models">Tool Series - BottyBot: A Frontend Chat UI for Local LLM Models</a>
</h2>
<p><a href="https://www.github.com/patw/BottyBot">https://www.github.com/patw/BottyBot</a></p>
<p>In this installment of our Generative AI (GenAI) tool series, we will be exploring a unique solution to interfacing with locally hosted Large Language Models (LLMs): BottyBot. Developed by an individual who was not satisfied with existing options on the market, BottyBot is specifically designed to seamlessly connect with <code>llama.cpp</code> running in server mode. This powerful frontend chat UI has become a crucial tool in the developer&rsquo;s daily workflow, serving as the main interface for interacting with multiple tools and applications that leverage the capabilities of the LLM.</p>
<p>The creator of BottyBot operates two GPU-accelerated instances of <code>llama.cpp</code>, which serve as the backbone for numerous applications such as SumBot, ExtBrain, RAGTAG (soon to be updated), and several Python scripts, including a website generator. These tools benefit from the efficiency and versatility provided by BottyBot&rsquo;s intuitive chat interface, which is typically powered by either the OpenHermes Mistral or Dolphin Mistral families of LLM models.</p>
<p>One of the key features that sets BottyBot apart is its support for multiple &ldquo;bot&rdquo; identities. These distinct personalities can be engaging to interact with and are entirely generated within the application itself. The development process of BottyBot exemplifies a unique approach known as &ldquo;bootstrapping,&rdquo; where much of the initial design was created using OpenAI&rsquo;s ChatGPT-3, while subsequent features were added by directly communicating with the LLM model integrated into BottyBot. This innovative method has resulted in a continually evolving and feature-rich application that caters to a wide range of use cases.</p>
<p>In addition to its core functionality, BottyBot also includes export capabilities for formatting and organizing conversations in an easily shareable format. This feature is particularly useful for collaborating with others or showcasing the results of interactions with LLM models.</p>
<p>While BottyBot does not currently support Retrieval Augmented Generation (RAG) techniques like RAGTAG or ExtBrain, its developers have expressed interest in potentially incorporating manual augmentation or vector search capabilities in future updates. This would allow users to enhance the prompt generation process and further optimize their interactions with LLM models.</p>
<p>Overall, BottyBot has proven to be an incredibly valuable tool for individuals who wish to harness the power of local, open-source Large Language Models while maintaining complete privacy and control over their data. As a result, it serves as a perfect example of how cutting-edge AI technology can be effectively integrated into everyday workflows and applications. Stay tuned for future updates and enhancements to this versatile and essential chat interface!</p>
<ul>
<li>Human Intervention: Minor.  Added the URL for the github repo up top.  Also, it seemed to depersonalize me entirely in this article talking about an unknown developer.  I&rsquo;m cool with it, but I probably needed to add some context in the points to indicate who worked on it.</li>
</ul>
<h3 id="facts-used">
  <a class="Heading-link u-clickable" href="/posts/tool-series-bottybot/#facts-used">Facts Used:</a>
</h3>
<ul>
<li>This series covers different tools used for building Generative AI (genai) applications</li>
<li>
<ul>
<li>Fifth Tool in the series is BottyBoy (<a href="https://www.github.com/patw/BottyBot">https://www.github.com/patw/BottyBot</a>)</li>
</ul>
</li>
<li>
<ul>
<li>BottyBot is a front end chat UI that connects to llama.cpp running in server mode which hosts a LLM (large language model)</li>
</ul>
</li>
<li>
<ul>
<li>I wasn&rsquo;t happy with other solutions on the market and none of them could consume llama.cpp in server mode directly.  I operate 2 GPU accelerated instances of llama.cpp which is used by multiple tools like SumBot, ExtBrain, RAGTAG (soon, it needs updating) and a few python scripts like my website generator</li>
</ul>
</li>
<li>
<ul>
<li>BottyBot has been a huge success for me, as I use it daily as my main chat interface to LLM models. The back end llama.cpp server is usually running the OpenHermes Mistral or Dolphin Mistral families of LLM models.</li>
</ul>
</li>
<li>
<ul>
<li>BottyBot supports different &ldquo;bot&rdquo; identities that represent differnet personalities that can be interesting to interact with.  The entire set of built in identites were generated by BottyBot!</li>
</ul>
</li>
<li>
<ul>
<li>BottyBot was a perfect example of bootstrapping:  I designed a lot of the application with OpenAI&rsquo;s ChatGPT 3, but as soon as the UI was running well enough, all features from that point on were added by talking to the LLM and getting useful python code for features I wanted.  It&rsquo;s now being used for all future products.</li>
</ul>
</li>
<li>
<ul>
<li>I added export functionality to produce nicely formatted exports for conversations.  These are useful for sharing with others.</li>
</ul>
</li>
<li>
<ul>
<li>BottyBot is not an example of RAG (retrieval augmented generation) like RAGTAG or ExtBrain.  BottyBoty uses the LLM directly without any augmentation.</li>
</ul>
</li>
<li>
<ul>
<li>A future enhancement to this tool could include manual augmentation or vector search for augmenting the LLM prompt.</li>
</ul>
</li>
<li>
<ul>
<li>I love this tool and so far, it&rsquo;s provided the most value to me personally and is a perfect example of using local, opensource large language models with full privacy.</li>
</ul>
</li>
</ul>

      
      


  

  





  <footer>
    
      
        <ul class="Tags">
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/categories/ai/" rel="tag">AI</a>
            </li>
          
        </ul>
      
    
      
        <ul class="Tags">
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/llm/" rel="tag"> LLM</a>
            </li>
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/tools/" rel="tag"> Tools</a>
            </li>
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/chatbot/" rel="tag"> Chatbot</a>
            </li>
          
        </ul>
      
    
  </footer>

    </article>
    
      <div class="Divider"></div>
    
  
    <article>
      <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="/posts/tool-series-instructorvec/" rel="bookmark">Tool Series - InstructorVec</a>
  </h2>
  
    <time datetime="2024-02-07T00:00:00Z">7 February, 2024</time>
  
</header>
      
        <h1 id="tool-series---instructorvec-a-single-endpoint-for-text-embedding">
  <a class="Heading-link u-clickable" href="/posts/tool-series-instructorvec/#tool-series---instructorvec-a-single-endpoint-for-text-embedding">Tool Series - InstructorVec: A Single Endpoint for Text Embedding</a>
</h1>
<p><a href="https://www.github.com/patw/InstructorVec">https://www.github.com/patw/InstructorVec</a></p>
<p>In this series, we dive into different tools and techniques used to build Generative AI applications. As we progress through the series, we&rsquo;ll explore a variety of methods that aid in creating efficient and powerful models. Today, we delve into InstructorVec, the latest evolution in the VectorService tool family.</p>
<p>InstructorVec is not just another text embedding model; it&rsquo;s an innovative approach to generating dense vectors for production use cases. It&rsquo;s designed as a single endpoint that calls the instructor-large model from HuggingFace, marking a departure from the previous VectorService tool that hosted eight different embedding endpoints. This change allows us to focus entirely on a singular solution for text embedding and outputting 768 dimension dense vectors, making it ideal for real-world applications.</p>
<p>To achieve this, InstructorVec loads the full instructor-large model but quantizes it from its original FP32 precision down to FP8. This slight reduction in precision offers significant performance improvements without compromising the quality of the outputs. In fact, executing InstructorVec takes no more than 100 milliseconds on a CPU, compared to the 1000 millisecond execution time at full FP32 precision. As a result, many RAG (Retrieval Augmented Generation) tools that consume this service have become much more responsive and efficient.</p>
<p>As we continue to build new production applications, InstructorVec will serve as the baseline vectorizer for all our tools moving forward. This is because its performance and precision make it a reliable solution for various use cases. Moreover, we currently maintain multiple copies of this service to cater to different production requirements.</p>
<p>In the future, we may expand InstructorVec&rsquo;s capabilities by adding a similarity checking endpoint. Similarity checking was one of the most useful features in the legacy VectorService tool, and its integration into InstructorVec will further enhance its versatility and utility.</p>
<p>To sum it up, InstructorVec is an innovative single-endpoint solution for text embedding that delivers high performance and precision while being ideal for production applications. As we continue to refine this tool, we&rsquo;re excited about the possibilities it holds for enhancing our Generative AI applications and unlocking new levels of efficiency in RAG tools. Stay tuned as we explore more exciting developments in the world of text embedding and Generative AI!</p>
<ul>
<li>Human Intervention: Minor.  I added the URL for the github repo.  Sometimes it adds it as a link in the text, other times it forgets it entirely.</li>
</ul>
<h3 id="facts-used">
  <a class="Heading-link u-clickable" href="/posts/tool-series-instructorvec/#facts-used">Facts Used:</a>
</h3>
<ul>
<li>
<ul>
<li>This series covers different tools used for building Generative AI (genai) applications</li>
</ul>
</li>
<li>
<ul>
<li>Fourth Tool in the series is InstructVec (<a href="https://www.github.com/patw/InstructorVec">https://www.github.com/patw/InstructorVec</a>)</li>
</ul>
</li>
<li>
<ul>
<li>InstructorVec is the next generation of VectorService tool, however instead of hosting 8 different embedding endpoints, it hosts a single endpoint that calls the instructor-large model from HuggingFace.</li>
</ul>
</li>
<li>
<ul>
<li>The other text embedding models in VectorService are nice to show off differences in vector outputs and can demonstrate how different models measure similarity between two strings of text.</li>
</ul>
</li>
<li>
<ul>
<li>InstructorVec is focused entirely on being a single endpoint for text embedding and outputting 768 dimension dense vectors for production use cases.</li>
</ul>
</li>
<li>
<ul>
<li>This tool loads the full instructor-large model, but quantizes it from full FP32 precision down to FP8.  For a small loss in precision in the model weights, it executes in 100 ms or less on CPU compared to 1000 ms at full precision.  This has made some RAG (retrieval augmented generation) tools that consume this service, much more responsive.</li>
</ul>
</li>
<li>
<ul>
<li>This will be the baseline vectorizer I&rsquo;ll be using in all tools moving forward, and currently operate multiple copies of this for servicing different production applications</li>
</ul>
</li>
<li>
<ul>
<li>Future state for this tool might include a similarity checking endpoint, as this has been very useful in the legacy VectorService tool.</li>
</ul>
</li>
</ul>

      
      


  

  





  <footer>
    
      
        <ul class="Tags">
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/categories/ai/" rel="tag">AI</a>
            </li>
          
        </ul>
      
    
      
        <ul class="Tags">
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/vector-search/" rel="tag"> Vector Search</a>
            </li>
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/tools/" rel="tag"> Tools</a>
            </li>
          
        </ul>
      
    
  </footer>

    </article>
    
      <div class="Divider"></div>
    
  
    <article>
      <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="/posts/tool-series-ragtag/" rel="bookmark">Tool Series - RAGTAG</a>
  </h2>
  
    <time datetime="2024-02-07T00:00:00Z">7 February, 2024</time>
  
</header>
      
        <h1 id="tool-series---ragtag-an-in-depth-look-at-retrieval-augmented-generation">
  <a class="Heading-link u-clickable" href="/posts/tool-series-ragtag/#tool-series---ragtag-an-in-depth-look-at-retrieval-augmented-generation">Tool Series - RAGTAG: An In-depth Look at Retrieval Augmented Generation</a>
</h1>
<p><a href="https://www.github.com/patw/RAGTAG">https://www.github.com/patw/RAGTAG</a></p>
<p>In our ongoing series covering various tools for building Generative AI applications, we dive into the third tool: <strong>RAGTAG</strong>. This end-to-end example of RAG (Retrieval Augmented Generation) allows you to experiment with question/answer pairs, test lexicographic and semantic search capabilities, and generate an LLM (Large Language Model) response using semantically augmented data.</p>
<p>As a simple CRUD application, RAGTAG enables users to create and modify question/answer pairs that are combined into a single chunk of text. This text is then run through the &ldquo;instruct-large&rdquo; text embedding model for retrieval later on. All semantic search in RAGTAG is performed using vector search with the same &ldquo;instruct-large&rdquo; model.</p>
<p>The key to RAGTAG&rsquo;s effectiveness lies in its tunable features. The chunk testing tool allows users to adjust the score cut-off for cosine similarity in vector search as well as control the K value, which determines the overrequest value on the vector search query. Meanwhile, the LLM tester provides an interface to set the above semantic search parameters along with system messages, prompts, and user questions.</p>
<p>RAGTAG is a product of Mongodb&rsquo;s specialist search group and has been in production since its inception. It serves as a valuable tool for question answering, demonstrating the power of RAG and its practical applications. However, there is room for improvement. The current LLM (implemented with the Llama.cpp Python module) runs on CPU instead of GPU, which can cause response generation to be slow.</p>
<p>Looking towards the future, we envision a more efficient version of RAGTAG that incorporates InstructorVec for text embedding and runs Llama.cpp in server mode. By leveraging these advancements, RAGTAG will be better equipped to share infrastructure with other tools and enjoy the benefits of GPU-accelerated token generation for faster response times.</p>
<p>In conclusion, RAGTAG is an essential tool for those looking to experiment with Retrieval Augmented Generation. With its robust capabilities and potential for improvement, it continues to be a valuable resource within our Generative AI toolkit. Stay tuned as we explore further advancements in this exciting field!</p>
<ul>
<li>Human Intervention: Added URL for github project and fixed InstructionVec to InstructorVec</li>
</ul>
<h3 id="facts-used">
  <a class="Heading-link u-clickable" href="/posts/tool-series-ragtag/#facts-used">Facts Used:</a>
</h3>
<ul>
<li>This series covers different tools used for building Generative AI (genai) applications</li>
<li>
<ul>
<li>Third Tool in the series is RAGTAG (<a href="https://www.github.com/patw/RAGTAG">https://www.github.com/patw/RAGTAG</a>)</li>
</ul>
</li>
<li>
<ul>
<li>RAGTAG is an end-to-end example of RAG (retrieval augmented generation) allowing you to manually create question/answer pairs, testing lexical and semantic search and allow you to generate an LLM (large language model) response with semantic search augmented data and manipulate the system message and the prompt to see how it changes the ouput.</li>
</ul>
</li>
<li>
<ul>
<li>This is a pretty simple CRUD application that lets you create and modify question/answer pairs which get appended together as a single chunk of text and run through the &ldquo;instructor-large&rdquo; text embedding model for retrieval later.</li>
</ul>
</li>
<li>
<ul>
<li>All semantic search is performed using vector search using the same instructor-large model</li>
</ul>
</li>
<li>
<ul>
<li>The chunk testing tool allows you to tune the score cut-off for the cosine similarity in the vector search as well as the K value, which controls the overrequest value on the vector search query</li>
</ul>
</li>
<li>
<ul>
<li>The LLM tester allows you to set the above semantic search values as well as the system message and the LLM prompt format along with the users question.</li>
</ul>
</li>
<li>
<ul>
<li>RAGTAG was a great experiment and is still used in production for question answering for our own specialist search group at MongoDB</li>
</ul>
</li>
<li>
<ul>
<li>This tool also integrated the text embedding model and the LLM into a single installable package, which was very convenient.  However, the LLM (running on llama.cpp python module) will run on CPU instead of GPU making the output responses quite slow</li>
</ul>
</li>
<li>
<ul>
<li>The future for this tool is to migrate it to the InstructorVec for text embedding and on llama.cpp running in server mode, so it can share infra with other tools and run on GPU for much faster token generation.</li>
</ul>
</li>
</ul>

      
      


  

  





  <footer>
    
      
        <ul class="Tags">
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/categories/ai/" rel="tag">AI</a>
            </li>
          
        </ul>
      
    
      
        <ul class="Tags">
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/rag/" rel="tag">RAG</a>
            </li>
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/chunking/" rel="tag"> Chunking</a>
            </li>
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/vector-search/" rel="tag"> Vector Search</a>
            </li>
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/tools/" rel="tag"> Tools</a>
            </li>
          
        </ul>
      
    
  </footer>

    </article>
    
      <div class="Divider"></div>
    
  
    <article>
      <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="/posts/tool-series-sumbot/" rel="bookmark">Tool Series - SumBot</a>
  </h2>
  
    <time datetime="2024-02-06T00:00:00Z">6 February, 2024</time>
  
</header>
      
        <h1 id="tool-series---sumbot-a-powerful-ai-summarization-tool-for-structured-data">
  <a class="Heading-link u-clickable" href="/posts/tool-series-sumbot/#tool-series---sumbot-a-powerful-ai-summarization-tool-for-structured-data">Tool Series - SumBot: A Powerful AI Summarization Tool for Structured Data</a>
</h1>
<p><a href="https://www.github.com/patw/sumbot">https://www.github.com/patw/sumbot</a></p>
<p>In our ongoing series covering various tools used for building Generative AI (genai) applications, we are excited to introduce you to SumBot, a Python FastAPI service designed specifically for summarizing structured data into semantically rich English text. As the second tool in this series, SumBot has proven its worth as an essential addition to any genai developer&rsquo;s toolbox, particularly when working with JSON or XML data.</p>
<h2 id="what-is-sumbot">
  <a class="Heading-link u-clickable" href="/posts/tool-series-sumbot/#what-is-sumbot">What is SumBot?</a>
</h2>
<p>SumBot is a powerful AI summarization tool that takes structured data (usually JSON) and converts it into coherent paragraphs of English text. With just a single endpoint (<code>summarize</code>) and two parameters - <code>entity</code> and <code>data</code>, this Python FastAPI service can quickly process and summarize your data, making it ideal for running through text embedding models like BERT or Instruct-large.</p>
<h2 id="why-use-sumbot">
  <a class="Heading-link u-clickable" href="/posts/tool-series-sumbot/#why-use-sumbot">Why Use SumBot?</a>
</h2>
<p>Embedding models often struggle to perform well on JSON, XML, point form data, or tabular data. By using an LLM (Large Language Model) for pre-summarization before text embedding, you can significantly improve recall and precision for semantic search. SumBot was the first tool I hosted on a GPU with LLaMA.cpp running in server mode, utilizing the OpenHermes-2.5-Mistral-7b model to provide accurate summarizations.</p>
<h2 id="how-does-sumbot-work">
  <a class="Heading-link u-clickable" href="/posts/tool-series-sumbot/#how-does-sumbot-work">How Does SumBot Work?</a>
</h2>
<p>The actual LLM prompt uses the <code>entity</code> parameter to guide the LLM into summarizing the JSON or XML data. This guidance can be necessary if the keys in your JSON document aren&rsquo;t clear enough for the LLM to figure out what it&rsquo;s summarizing. Thankfully, SumBot doesn&rsquo;t require validation of whether the input data is actually JSON or XML; it can summarize almost anything as long as you provide it an <code>entity</code> and <code>data</code>.</p>
<h2 id="deploying-sumbot">
  <a class="Heading-link u-clickable" href="/posts/tool-series-sumbot/#deploying-sumbot">Deploying SumBot</a>
</h2>
<p>SumBot can be deployed against any LLaMA.cpp server running locally or could be easily updated to point to a hosted service like Mistral.ai or OpenAI. This flexibility makes SumBot an excellent choice for developers who need to quickly process and summarize large amounts of structured data while taking advantage of the latest advancements in AI technology.</p>
<p>In conclusion, SumBot is a valuable addition to any genai developer&rsquo;s toolbox. Its ability to transform JSON or XML data into coherent English text using LLM pre-summarization makes it an essential tool for improving recall and precision in semantic search. As the second installment in our series on tools for building Generative AI applications, SumBot demonstrates the power of leveraging cutting-edge technology to optimize workflows and enhance productivity.</p>
<ul>
<li>Human Intervention: Minor.  I had to add the Github URL to sumbot to the article.</li>
</ul>
<h3 id="facts-used">
  <a class="Heading-link u-clickable" href="/posts/tool-series-sumbot/#facts-used">Facts Used:</a>
</h3>
<ul>
<li>This series covers different tools used for building Generative AI (genai) applications</li>
<li>
<ul>
<li>Second tool in the series is SumBot (<a href="https://www.github.com/patw/sumbot">https://www.github.com/patw/sumbot</a>)</li>
</ul>
</li>
<li>
<ul>
<li>SumBot is used for summarizing structured data (usually JSON) into paragraphs of sementically rich english text.</li>
</ul>
</li>
<li>
<ul>
<li>The tool is a Python FastAPI service with a single endpoint (summarize) and two parameters: entity and data</li>
</ul>
</li>
<li>
<ul>
<li>The output of SumBot is ideal for running through text embedding models like BERT or Instructor-large</li>
</ul>
</li>
<li>
<ul>
<li>Embedding models tend to perform poorly on JSON, XML, point form data and tabular data.  Using an LLM (large language model) for pre-summarization before text embedding can provide a drastic increase in recall and precision for semantic search</li>
</ul>
</li>
<li>
<ul>
<li>SumBot was the first tool I hosted on GPU with llama.cpp running in server mode with the OpenHermes-2.5-Mistral-7b model.</li>
</ul>
</li>
<li>
<ul>
<li>The actual LLM prompt uses the entity parameter to guide the LLM into summarizing the JSON or XML data.  This can be necessary if the keys in the JSON document aren&rsquo;t clear enough for the LLM to figure out what it&rsquo;s summarizing.</li>
</ul>
</li>
<li>
<ul>
<li>SumBot doesn&rsquo;t validate if it&rsquo;s actually JSON or XML data! It can be used to summarize almost anything, as long as you provide it an entity and data.</li>
</ul>
</li>
<li>
<ul>
<li>This tool can be deployed against any llama.cpp server running locally or could be easily updated to point to a hosted service like Mistral.ai or OpenAI</li>
</ul>
</li>
</ul>

      
      


  

  





  <footer>
    
      
        <ul class="Tags">
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/categories/ai/" rel="tag">AI</a>
            </li>
          
        </ul>
      
    
      
        <ul class="Tags">
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/llm/" rel="tag"> LLM</a>
            </li>
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/chunking/" rel="tag"> Chunking</a>
            </li>
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/presum/" rel="tag"> Presum</a>
            </li>
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/llama.cpp/" rel="tag"> llama.cpp</a>
            </li>
          
        </ul>
      
    
  </footer>

    </article>
    
      <div class="Divider"></div>
    
  
    <article>
      <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="/posts/tool-series-vectorservice/" rel="bookmark">Tool Series - VectorService</a>
  </h2>
  
    <time datetime="2024-02-06T00:00:00Z">6 February, 2024</time>
  
</header>
      
        <h1 id="tool-series---vectorservice-exploring-the-journey-of-text-embedding-models">
  <a class="Heading-link u-clickable" href="/posts/tool-series-vectorservice/#tool-series---vectorservice-exploring-the-journey-of-text-embedding-models">Tool Series - VectorService: Exploring the Journey of Text Embedding Models</a>
</h1>
<p>In this series, we will dive deep into different tools used for building Generative AI (GenAI) applications. The first tool in our exploration is <a href="https://www.github.com/patw/VectorService">VectorService</a>, a FastAPI service that generates dense vectors using various text embedding models.</p>
<p>The initial motivation behind VectorService was to test out multiple different text embedding models for generating semantic search capabilities for RAG (Retrieval Augmented Generation) tools with LLMs (Large Language Models). The journey of exploring and implementing various embedding models in this tool has taught us valuable lessons about the evolution of language processing techniques.</p>
<h2 id="from-spacy-to-sentencetransformers-a-journey-of-improvement">
  <a class="Heading-link u-clickable" href="/posts/tool-series-vectorservice/#from-spacy-to-sentencetransformers-a-journey-of-improvement">From SpaCY to SentenceTransformers: A Journey of Improvement</a>
</h2>
<p>VectorService&rsquo;s initial models were sourced from the Python <a href="https://spacy.io/">SpaCY Library</a>. We implemented small (96d), medium (384d), and large (384d) SpaCY models, which proved to be quite easy to use. However, these models performed poorly beyond a few words or a single sentence compared to more modern alternatives like BERT. They remain in use for legacy reasons in some applications.</p>
<p>To improve the quality of text embeddings, we then moved on to using the <a href="https://www.sbert.net/">SentenceTransformers Library</a>, which was incredibly easy to work with. The library provided two models: all-MiniLM-L6-v2 (384d) and all-mpnet-base-v2 (768d). These models performed significantly better than SpaCY, demonstrating the advancements in language processing techniques over time. Many RAG examples online still show miniLM as the text embedding model of choice; however, it&rsquo;s worth noting that larger models like mpnet-all-MiniLM-L6-cos-v1 outperform it significantly.</p>
<h2 id="bert-a-significant-leap-in-quality">
  <a class="Heading-link u-clickable" href="/posts/tool-series-vectorservice/#bert-a-significant-leap-in-quality">BERT: A Significant Leap in Quality</a>
</h2>
<p>Next, we explored using <a href="https://arxiv.org/abs/1810.04805">BERT</a> (Bidirectional Encoder Representations from Transformers), a groundbreaking language processing model developed by Google. BERT uses 768 dimensions and delivered a substantial improvement in the quality of text embeddings compared to its predecessors. The recall and precision metrics showed significant jumps, indicating that BERT was a major step forward for text embedding models.</p>
<h2 id="sota-instructor-large-takes-center-stage">
  <a class="Heading-link u-clickable" href="/posts/tool-series-vectorservice/#sota-instructor-large-takes-center-stage">SOTA: Instructor-Large Takes Center Stage</a>
</h2>
<p>The final model we integrated into VectorService was <a href="https://huggingface.co/hkunlp/instructor-large">Instructor-Large</a>, a state-of-the-art (SOTA) language processing model at the time of its release. This model achieved exceptional results on the HuggingFace MTEB leaderboard and required 4 gigabytes of memory to run, making it quite slow on CPUs.</p>
<p>However, the quality level of Instructor-Large was considered the bare minimum for production use cases, and it could be quantized to reduce its memory footprint and latency. This model required LLM-style prompting to produce optimal results and directly competed with OpenAI&rsquo;s much larger text-ada-002 model, which is typically used as a default in RAG applications.</p>
<h2 id="a-comparison-of-models-the-power-of-benchmarking">
  <a class="Heading-link u-clickable" href="/posts/tool-series-vectorservice/#a-comparison-of-models-the-power-of-benchmarking">A Comparison of Models: The Power of Benchmarking</a>
</h2>
<p>To provide users with valuable insights into the performance of different models, VectorService included endpoints for comparing similarity results across all text embedding models implemented. This feature allowed users to benchmark recall and precision metrics between various models, which in turn helped them optimize their RAG use cases.</p>
<h2 id="the-evolution-of-text-embedding-models-from-legacy-to-state-of-the-art">
  <a class="Heading-link u-clickable" href="/posts/tool-series-vectorservice/#the-evolution-of-text-embedding-models-from-legacy-to-state-of-the-art">The Evolution of Text Embedding Models: From Legacy to State-of-the-Art</a>
</h2>
<p>As we have seen, the journey from SpaCY to Instruction-Large showcases the evolution of text embedding models and how they have improved over time. VectorService served as a valuable experimentation platform for exploring different models and their capabilities. However, it is now considered legacy and not recommended for use. Instead, we recommend using <a href="https://www.github.com/InstructorVec">InstrucTorVec</a>, an open-source alternative that offers self-hosted vector embedding solutions with exceptional performance and ease of use.</p>
<p>In conclusion, the development of VectorService has been a fascinating journey through the world of text embedding models. From SpaCY&rsquo;s early attempts to BERT&rsquo;s groundbreaking achievements and Instruction-Large&rsquo;s SOTA status, we have witnessed the incredible progress made in language processing technologies. As we move forward into an era of increasingly sophisticated AI applications, it is essential for developers and researchers alike to continue exploring new frontiers in this rapidly evolving field.</p>
<ul>
<li>Human Intervention: Moderate: the bot continues to call the Instructor model Instruction and totally made up a research paper for it.  I also had to correct the two sentence transformer model names, but that was entirely my fault for providing the wrong names in the facts.  I wrote it on a plane while I was half asleep.</li>
</ul>
<h3 id="facts-used">
  <a class="Heading-link u-clickable" href="/posts/tool-series-vectorservice/#facts-used">Facts Used:</a>
</h3>
<ul>
<li>
<ul>
<li>This series covers different tools used for building Generative AI (genai) applications</li>
</ul>
</li>
<li>
<ul>
<li>First tool in the series is VectorService (<a href="https://www.github.com/patw/VectorService">https://www.github.com/patw/VectorService</a>)</li>
</ul>
</li>
<li>
<ul>
<li>Text embedding models that output dense vectors and critical for building semantic search for RAG (retrieval augmented generation) tools with LLMs (large language models)</li>
</ul>
</li>
<li>
<ul>
<li>Originally wanted to test out multiple different text embedding models, so built a FastAPI service in Python that would generate dense vectors using different models.</li>
</ul>
</li>
<li>
<ul>
<li>The first 3 models were from the Python SpaCY Library. I implemented small (96d), medium (384d) and large (384d).  SpaCY was very easy to use, but performed pretty poorly beyond a few words or a single sentence, compared to more modern models like BERT.  They&rsquo;re still in the process for legacy reasons.</li>
</ul>
</li>
<li>
<ul>
<li>The next two models were minilm-l6 (384d) and mpnet-alllm (768d).  These models performed much better than SpaCY and used the HuggingFace SentenceTransformers library, which was super easy to use.  Many RAG examples online still show minilm as the text embedding model, and while it performs poorly compared to larger models it&rsquo;s still quite good.</li>
</ul>
</li>
<li>
<ul>
<li>Next, I tried BERT (768d).  This model used 768 dimensions and seemed to be another large step up in quality for embeddings and was the first time I saw large jumps in recall and precision.  BERT has much more dimensions than minilm, but performed better in all my tests</li>
</ul>
</li>
<li>
<ul>
<li>Finally I added in Instructor-large (768d).  This model was considered SOTA (state of the art) for the time it released and quickly became #1 on the Huggingface MTEB leaderboard.  The model itself needed 4 gigs of memory to run and is quite slow on CPU.  However, the quality level should be considered the bare minimum for production use cases, and can be quantized to run with less memory and less latency. Instructor requires LLM style prompting to produce good results and competes directly with OpenAI&rsquo;s much larger text-ada-002 model, which is the default for most RAG use cases.</li>
</ul>
</li>
<li>
<ul>
<li>This tool also included endpoints for comparing similarity results across all the models, which is useful to show off in a demo.  It gives customers the idea that they should be benchmarking recall and precision between multiple models to optimize the RAG use case.</li>
</ul>
</li>
<li>
<ul>
<li>At this point, VectorService is legacy and not recommended.  InstructorVec (<a href="https://www.github.com/InstructorVec">https://www.github.com/InstructorVec</a>) is considered a replacement for this tool.  It was a great exercise for experimenting with different embedding models but InstructorVec is all you need for self hosted, open source vector embeddings.</li>
</ul>
</li>
</ul>

      
      


  

  





  <footer>
    
      
        <ul class="Tags">
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/categories/ai/" rel="tag">AI</a>
            </li>
          
        </ul>
      
    
      
        <ul class="Tags">
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/vector-search/" rel="tag"> Vector Search</a>
            </li>
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/tools/" rel="tag"> Tools</a>
            </li>
          
        </ul>
      
    
  </footer>

    </article>
    
      <div class="Divider"></div>
    
  
    <article>
      <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="/posts/chunking-techniques-llm-presummarization/" rel="bookmark">Chunking Techniques - LLM Presummarization</a>
  </h2>
  
    <time datetime="2024-02-05T00:00:00Z">5 February, 2024</time>
  
</header>
      
        <h1 id="chunking-techniques---llm-presummarization">
  <a class="Heading-link u-clickable" href="/posts/chunking-techniques-llm-presummarization/#chunking-techniques---llm-presummarization">Chunking Techniques - LLM Presummarization</a>
</h1>
<p>Choosing a good chunking strategy for unstructured text documents is crucial to the success of your Retrieval Augmented Generation (RAG) use case. In this post, we will discuss the use of Large Language Models (LLMs) for pre-summarizing structured data, with the aim of producing semantically rich paragraphs that are ideal inputs for text embedding models and semantic search recall.</p>
<h2 id="the-challenges-of-structured-data">
  <a class="Heading-link u-clickable" href="/posts/chunking-techniques-llm-presummarization/#the-challenges-of-structured-data">The Challenges of Structured Data</a>
</h2>
<p>Creating dense vector embeddings with structured data like XML and JSON often results in weak embeddings due to repetitive keys and control characters, which can cause poor recall and precision when performing semantic searches. Similar issues can be encountered with tabular data, point form data, tables of contents, and appendixes found in regular documents. In this post, we will focus on JSON documents specifically.</p>
<h2 id="llm-pre-summarization-method">
  <a class="Heading-link u-clickable" href="/posts/chunking-techniques-llm-presummarization/#llm-pre-summarization-method">LLM Pre-Summarization Method</a>
</h2>
<p>To address these challenges, we propose using the LLM pre-summarization method, where your original JSON document is fed to the LLM, asking for a one-paragraph summary of the record itself. The output of this summarized record is then sent to the text embedding model for vectorization. This approach can provide significantly improved recall and precision compared to other chunking methods we have discussed in previous posts.</p>
<h2 id="cost-considerations">
  <a class="Heading-link u-clickable" href="/posts/chunking-techniques-llm-presummarization/#cost-considerations">Cost Considerations</a>
</h2>
<p>The downside of the LLM pre-summarization method is its cost, as running an entire set of structured or unstructured data through the LLM for summarization can be expensive, especially when using commercial models like OpenAI&rsquo;s GPT-4. To mitigate costs, we recommend testing less expensive alternatives such as GPUs or services like Mistral.ai and the &ldquo;mistral-tiny&rdquo; model.</p>
<h2 id="future-trends-in-chunking-strategies">
  <a class="Heading-link u-clickable" href="/posts/chunking-techniques-llm-presummarization/#future-trends-in-chunking-strategies">Future Trends in Chunking Strategies</a>
</h2>
<p>For projects with larger budgets, LLM pre-summarization will likely be the best choice for chunking strategies. In the future, almost all use cases will require some level of data summarization like &ldquo;Fact Synthesis&rdquo; mentioned earlier in this series. By employing techniques such as LLM pre-summarization, you can better align structured data with text embedding models, enabling more effective semantic search capabilities against structured data.</p>
<ul>
<li>Human Intervention: None</li>
</ul>
<h3 id="facts-used">
  <a class="Heading-link u-clickable" href="/posts/chunking-techniques-llm-presummarization/#facts-used">Facts Used:</a>
</h3>
<ul>
<li>Choosing a good chunking strategy for your unstructured text documents (pdf, word, html) is critical to the success of your RAG (retrieval augmented generation) use case.</li>
<li>Chunks of text, in this case are what is sent to the text embedding model, which produces dense vectors which are searched for similarity using vector search.  Chunks returned are sent to the LLM (large language model), usually to answer questions.</li>
<li>There is no one size fits all strategy to text chunking, however we have observed many different strategies in the field.  You should try each one and benchmark it for recall and precision with your embedding model of choice, or experiment with multiple embedding models against each chunking method until you get the best possible recall.</li>
<li>As we get further into this series, the level of sophistication of the techniques will increase.</li>
<li>In previous chunking methods in this series, the text chunk was always sent to the LLM, unmodified after retrieval.  In this method we will use the LLM itself to pre-summarize the source text.  This ensures that if the data is structured, poorly worded or overly verbose you can output a semantically rich paragraph of english text, which is the ideal input for a text embedding model and semantic search recall.</li>
<li>The ninth in our series of posts about Chunking techniques we will discuss pre-summarizing structured data, using an LLM for the purpose of producing very strong text embeddings.</li>
<li>Creating dense vector embeddings with structured data like XML and JSON tends to produce really weak embeddings that suffer from poor recall and even worse precision.  This happens because structured data has lots of repeating keys and control characters.  Text embedding models tend to perform best against semantically rich paragraphs of plain english text.</li>
<li>This same problem can happen with tabular data, point form data, tables of content and appendixes on regular documents as well.  In this blog post we will address JSON documents specifically.</li>
<li>Using the LLM pre-sum method, you will feed your original JSON document to the LLM asking for a one paragraph summary of the record itself.  The output of this record should be sent to the text embedding model for vectorization.  This summarized version of the record will have dramatically better recall and precision against natural language queries/prompts than any other chunking method we’ve discussed in this series.</li>
<li>The downside of LLM pre-sum is the cost.  You need to run your entire set of structured or unstructured data through the LLM to summarize it first.  This can be quite expensive if you’re using commercial LLMs like GPT4 from OpenAI.  Test with the less expensive GPT3-turbo models first to see if the summaries it produces are good enough for your use case.  Even better, try services like Mistral.ai and the “mistral-tiny” model, which are 10x cheaper than even the GPT3-turbo model.</li>
<li>If money and budget is not a concern for your project, this will be the best possible choice for chunking strategies.  In the future, almost all use cases will require some level of data summarization like the “Fact Synthesis” blog post from earlier in this series.</li>
<li>This technique should provide a better match between structured data and text embedding models, allowing you to perform semantic search against structured data.</li>
</ul>

      
      


  

  





  <footer>
    
      
        <ul class="Tags">
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/categories/ai/" rel="tag">AI</a>
            </li>
          
        </ul>
      
    
      
        <ul class="Tags">
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/rag/" rel="tag">RAG</a>
            </li>
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/chunking/" rel="tag"> Chunking</a>
            </li>
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/vector-search/" rel="tag"> Vector Search</a>
            </li>
          
        </ul>
      
    
  </footer>

    </article>
    
      <div class="Divider"></div>
    
  
    <article>
      <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="/posts/chunking-techniques-static-text-generation-from-structured-data/" rel="bookmark">Chunking Techniques - Static Text Generation from Structured Data</a>
  </h2>
  
    <time datetime="2024-02-04T00:00:00Z">4 February, 2024</time>
  
</header>
      
        <h1 id="chunking-techniques-static-text-generation-from-structured-data">
  <a class="Heading-link u-clickable" href="/posts/chunking-techniques-static-text-generation-from-structured-data/#chunking-techniques-static-text-generation-from-structured-data">Chunking Techniques: Static Text Generation from Structured Data</a>
</h1>
<p>Choosing a good chunking strategy for your unstructured text documents is critical to the success of your Retrieval Augmented Generation (RAG) use case. Chunks of text, in this context, are the segments that are sent to the text embedding model, which produces dense vectors that are searched for similarity using vector search. The chunks returned are then sent to the Large Language Model (LLM), typically to answer questions.</p>
<p>There is no one-size-fits-all strategy to text chunking; however, we have observed many different strategies in the field. You should try each approach and benchmark it for recall and precision with your chosen embedding model or experiment with multiple embedding models against each chunking method until you achieve the best possible recall.</p>
<p>As we delve deeper into this series, the level of sophistication of the techniques will increase. In previous chunking methods in this series, the text chunk was always sent to the LLM without modification after retrieval. In this method, we will use a script to summarize structured data, such as a JSON document from a Mongo collection, into a paragraph of English text. We embed the summarized text but send the original JSON structured document to the LLM prompt to answer the user&rsquo;s question.</p>
<p>In this eighth post in our series about Chunking techniques, we will discuss pre-summarizing structured data using a script and running the summary text through a text embedding model for vector search retrieval later. Creating dense vector embeddings with structured data like XML and JSON often produces weak embeddings that suffer from poor recall and even worse precision due to the abundance of repeating keys and control characters, which are not semantically rich in plain English text. This same problem can occur with tabular data, point-form data, tables of contents, and appendixes on regular documents as well.</p>
<p>Converting a JSON document into a paragraph of English text requires a writing script that consumes all the fields and tries to tell a story about the data. Typically, this is done using a story template where the values of the fields get filled in. This process is similar to creating a MadLib, where the nouns and verbs are the values in the source document. The story output will then be run through your text embedding model to store the dense vectors that represent it. Additionally, you will store the original Mongo document ID for the source record along with it, so when you perform a vector search match afterward, you can retrieve the original document and send it to the LLM as part of the prompt augmentation.</p>
<p>This technique should provide a better match between structured data and text embedding models, allowing you to perform semantic search against structured data. This approach is also significantly cheaper to implement than LLM Structured Data Summarization, which we will discuss in a future blog post.</p>
<ul>
<li>Human Intervention: None</li>
</ul>
<h3 id="facts-used">
  <a class="Heading-link u-clickable" href="/posts/chunking-techniques-static-text-generation-from-structured-data/#facts-used">Facts Used:</a>
</h3>
<ul>
<li>Choosing a good chunking strategy for your unstructured text documents (pdf, word, html) is critical to the success of your RAG (retrieval augmented generation) use case.</li>
<li>Chunks of text, in this case are what is sent to the text embedding model, which produces dense vectors which are searched for similarity using vector search.  Chunks returned are sent to the LLM (large language model), usually to answer questions.</li>
<li>There is no one size fits all strategy to text chunking, however we have observed many different strategies in the field.  You should try each one and benchmark it for recall and precision with your embedding model of choice, or experiment with multiple embedding models against each chunking method until you get the best possible recall.</li>
<li>As we get further into this series, the level of sophistication of the techniques will increase.</li>
<li>In previous chunking methods in this series, the text chunk was always sent to the LLM, unmodified after retrieval.  In this method we will use a script to summarize structured data, like a JSON document from a Mongo collection, into a paragraph of english text.  We embed the summarized text, but we will send the original JSON structured document to the LLM prompt to answer the user&rsquo;s question.</li>
<li>The eighth in our series of posts about Chunking techniques we will discuss pre-summarizing structured data, using a script, and running the summary text through a text embedding model for vector search retrieval later.</li>
<li>Creating dense vector embeddings with structured data like XML and JSON tends to produce really weak embeddings that suffer from poor recall and even worse precision.  This happens because structured data has lots of repeating keys and control characters.  Text embedding models tend to perform best against semantically rich paragraphs of plain english text.</li>
<li>This same problem can happen with tabular data, point form data, tables of content and appendixes on regular documents as well.  In this blog post we will address JSON documents specifically.</li>
<li>Converting a JSON document into a paragraph of english text requires a writing script that consumes all the fields and tries to tell a story about the data.  Typically this is done with a story template, where the values of the fields get filled in.  This is similar to creating a MadLib, where the nouns and verbs are the values in the source document.</li>
<li>The story output will then be run through your text embedding model to store the dense vectors that represent it.  You will also store the original mongo document ID for the source record along with it, so when you perform a vector search match afterwards you can retrieve the original document and send it to the LLM, as part of the prompt augmentation.</li>
<li>This technique should provide a better match between structured data and text embedding models, allowing you to perform semantic search against structured data.</li>
<li>This technique is also significantly cheaper to implement than LLM Structured Data Summarization, which we will talk about in a future blog post.</li>
</ul>

      
      


  

  





  <footer>
    
      
        <ul class="Tags">
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/categories/ai/" rel="tag">AI</a>
            </li>
          
        </ul>
      
    
      
        <ul class="Tags">
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/rag/" rel="tag">RAG</a>
            </li>
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/chunking/" rel="tag"> Chunking</a>
            </li>
          
            <li class="Tags-item u-background">
              <a class="Tags-link u-clickable" href="/tags/vector-search/" rel="tag"> Vector Search</a>
            </li>
          
        </ul>
      
    
  </footer>

    </article>
    
  

  
  <nav>
    
      <a class="Pagination u-clickable" href="/page/2/" rel="prev">« Previous</a>
    
    
  </nav>




      </div>
    </div>
  </main>
  
  <footer class="Footer">
    <div class="u-wrapper">
      <div class="u-padding u-noboosting">
        Copyright 2024 Pat Wendorf (pat.wendorf@mongodb.com)
      </div>
    </div>
  </footer>

</body>

</html>
